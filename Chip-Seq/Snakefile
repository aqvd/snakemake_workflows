import os
import  pandas as pd
import numpy as np
import glob
import re

## write .yaml configuration filename
configfile: "config/test_config.yaml"

FASTQDIR=config["fastqdir"]
DATADIR=config["datadir"]
RESDIR=config["resdir"]
LOGDIR=config["logdir"]
TABLE_NAME=config["tableName"]

## Function to create directories unless they already exist
def tryMkdir(path):
	try:
		os.makedirs(path,exist_ok=False) # raise error if exists
	except FileExistsError:
		pass

[tryMkdir(p) for p in (DATADIR, RESDIR, LOGDIR, RESDIR+"/fastQC")]

## Genome bowtie2 index prefixes paths
genome_path = {
	"mm9":"/storage/scratch01/users/dgimenezl/genomes/mouse/mm9/mm9",
    "mm10":"/storage/scratch01/users/dgimenezl/genomes/mouse/mm10/mm10" ,
    "hg19":"/storage/scratch01/users/dgimenezl/genomes/human/hg19/hg19",
    "hg38":"/storage/scratch01/users/dgimenezl/genomes/human/hg38/hg38",
    "-":""}
refSeq_genes_path = {
	"mm9" : "",
	"mm10" : "",
	"hg19" : "/storage/scratch01/users/aquevedo/genomes/human/hg19/hg19_RefSeqCuratedGenes.bed",
	"hg38" : ""
}
## Genome sizes for big wig computation
genome_size={"mm9":2620345972,
    "mm10":2652783500,
    "hg19":2864785220,
    "hg38":2913022398}

## Read experiment data
data = pd.read_csv(TABLE_NAME,sep="\t")
## Add extra cols for salecting the appropriate wildcards path to files

data["Samples"] = data.Protein +"_"+data.Condition+"_"+ data.Rep 
# Match each sample reads with appropriate input to calculate calibration factor
data["Input"] = [ data.Samples[(data.Protein=="input") & (data.Condition==Cond)].values[0] \
                 if Prot != "input" \
                 else "" \
                 for Prot,Cond in zip(data.Protein,data.Condition)  ]

data["PATH_genome"] = [genome_path[i] for i in data.Genome] 

data["Genome_size"] = [genome_size[i] for i in data.Genome]

data["PATH_genome_cal"] = [genome_path[i] for i in data.Norm]

data["PATH_refSeq_genes"] = [refSeq_genes_path[i] for i in data.Genome]
## Remove .fastq.gz to use basename with expand() in rule "all"
data["fqBasename"] = [f.replace(".fastq.gz","") for f in data["File"]]

## function to parse config.yaml and extract resource parameters
def get_resource(rule,resource):
    try:
        return config["rules"][rule]["res"][resource]
    except KeyError:
        return config["rules"]["default"]["res"][resource]

## ## Rules priority
ruleorder: bowtie2_calibrated_mapping > bowtie2_align 

rule all:
	input:
		## fastQC quality control
		expand(RESDIR+"/fastQC/{fq_base}_fastqc.html", 
			fq_base=data.fqBasename.unique()),
		## fastQC quality control
		expand(RESDIR+"/fastQC/{fq_base}_fastqc.zip", 
			fq_base=data.fqBasename.unique()),
		## fastQ Screen contamination control
		expand(RESDIR+"/fastQScreen/{fq_base}_screen.txt", 
			fq_base=data.fqBasename.unique()),
		## fastQ Screen contamination control
		expand(RESDIR+"/fastQScreen/{fq_base}_screen.html", 
			fq_base=data.fqBasename.unique()),

		## .sam bowtie2 alignments
		#expand(DATADIR+"/align/{sample}.sam", 
		#	sample=data.Samples.unique()),
		## bowtie2 mapping stats 
		#expand(DATADIR + "/align/stats/{sample}.txt",
		#	sample=data.Samples.unique()),
		## bowtie2 maping againtst calibraton genome stats
		#expand(DATADIR+"/align/stats/{sample}_calibration.txt",
		#	sample=data.Samples[ data.PATH_genome_cal != "" ].unique()),
		## Calibration factors. Only for chip samples with input to compare
		#expand(DATADIR + "/align/{sample}_ORI_factor.txt",
		#	sample=data.Samples[data.Input != ""].unique()),
		## .bam alignment files sorted and without duplicates
		#expand(DATADIR+"/align/{sample}_final.bam", 
		#	sample=data.Samples.unique()),
		## Indexed .bai alignments
		#expand(DATADIR + "/align/{sample}_final.bai",
		#	sample=data.Samples.unique()),
		## .bw files RPKM normalized
		#expand(RESDIR+"/bw/{sample}_RPKM.bw", 
		#	sample=data.Samples.unique()),
		## .bw files scaled (CPM * scaleFactor)

		expand(RESDIR + "/bw/{sample}_RPKM_scaled.bw",
			sample=data.Samples[(data.PATH_genome_cal!="") & 
								(data.Protein!="input")].unique()),
		## Macs2 fragment prediction
		expand(RESDIR + '/macs/{sample}_predictd.txt',
			sample=data.Samples[data.Protein!="input"].unique()),
		## Macs2 peak files
		expand(RESDIR + '/macs/{sample}_peaks.xls',
			sample=data.Samples[data.Protein!="input"].unique()),
		## deeptools computeMatrix out files. TSS scaled
		RESDIR + "/deeptools/matrix_all_samples.gz",
		RESDIR + "/deeptools/regions_all_samples_TSS_sorted.bed"

rule QC_only:
	input:
		### fastQC quality control
		expand(RESDIR+"/fastQC/{fq_base}_fastqc.html", 
			fq_base=data.fqBasename.unique()),
		## fastQC quality control
		expand(RESDIR+"/fastQC/{fq_base}_fastqc.zip", 
			fq_base=data.fqBasename.unique()),
		## fastQ Screen contamination control
		expand(RESDIR+"/fastQScreen/{fq_base}_screen.txt", 
			fq_base=data.fqBasename.unique()),
		## fastQ Screen contamination control
		expand(RESDIR+"/fastQScreen/{fq_base}_screen.html", 
			fq_base=data.fqBasename.unique())

rule fastQC:
	input:
		# .fastq raw reads
		fq=FASTQDIR + '/{fq_base}.fastq.gz'
	output:
		RESDIR + "/fastQC/{fq_base}_fastqc.html",
		RESDIR + "/fastQC/{fq_base}_fastqc.zip"
	log:
		LOGDIR + "/fastQC_{fq_base}.log"
	threads:
		get_resource("fastQC", "threads")
	shell:
		"fastqc --outdir {}/fastQC --threads {{threads}} {{input.fq}} \
		|& tee {{log}}".format(RESDIR)

rule fastQScreen:
	input:
		fq=FASTQDIR + '/{fq_base}.fastq.gz'
	output:
		RESDIR+"/fastQScreen/{fq_base}_screen.txt",
		RESDIR+"/fastQScreen/{fq_base}_screen.html"
	log:
		LOGDIR + "/fastQC_{fq_base}.log"
	threads:
		get_resource("fastQScreen", "threads")
	shell:
		"fastq_screen --threads {{threads}} --aligner bowtie2 \
		--conf /home/aquevedo/opt/FastQ-Screen-0.14.1/fastq_screen.conf \
		--outdir {}/fastQScreen {{input.fq}} |& tee {{log}}".format(RESDIR)
		

rule bowtie2_align:
	input:
		fq=lambda wildcards: expand(FASTQDIR + '/{fq_file}', 
			fq_file=data.File[data.Samples==wildcards.sample].values)
	output:
		sam=temp(DATADIR + "/align/{sample}.sam"),
		stats=DATADIR + "/align/stats/{sample}.txt"
	params:
		## with logical indexing retrieve the same PATH_genome n times, 
		## get the first.
		genomeIndex= lambda wildcards: expand("{genome}",
			genome=data.PATH_genome[data.Samples==wildcards.sample].values[0])
	threads:
		get_resource("bowtie2", "threads")
	resources:
		mem_mb=get_resource("bowtie2", "mem_mb")	
	log:
		LOGDIR + "/bowtie2_{sample}.log"
	run:
		reads=",".join(input.fq),
		shell("bowtie2 -x {params.genomeIndex} -U {reads} \
			-p {threads} --time	--no-unal -S {output.sam} \
			|& tee {output.stats}")

rule bowtie2_calibrated_mapping:
	input:
		fq=lambda wildcards: expand(FASTQDIR + '/{fq_file}', 
			fq_file=data.File[data.Samples==wildcards.sample].values)
	output:
		sam=temp(DATADIR + "/align/{sample}_calibration.sam"),
		stats= DATADIR + "/align/stats/{sample}_calibration.txt"
	params:
		calGenIx = lambda wildcards: expand("{calGenome}",
			calGenome=data.PATH_genome_cal[data.Samples==wildcards.sample].values[0])
	threads:
		get_resource("bowtie2", "threads")
	resources:
		mem_mb=get_resource("bowtie2", "mem_mb")	
	log:
		LOGDIR + "/bowtie2_calibration_{sample}.log"
	run:
		reads=",".join(input.fq),
		shell("bowtie2 -x {params.calGenIx} -U {reads} \
			-p {threads} --time	--no-unal -S {output.sam} \
			|& tee {output.stats}")

rule calculate_scaled:
	input:
		# stats of alignment for Chip samples
		stats=DATADIR + "/align/stats/{sample}.txt",
		calStats=DATADIR + "/align/stats/{sample}_calibration.txt",
		# stats of alignment for Input samples
		inputStats=lambda wildcards: expand(
			DATADIR+"/align/stats/{input_stats}.txt", 
			input_stats=data.Input[data.Samples==wildcards.sample].values[0]),
		inputCalStats=lambda wildcards: expand(
			DATADIR+"/align/stats/{input_stats}_calibration.txt",
			input_stats=data.Input[data.Samples==wildcards.sample].values[0])
	output:
		DATADIR + "/align/{sample}_ORI_factor.txt"
	shell:
		"n1=$(awk -f scripts/get_aligned_reads.awk {input.stats});"
		"n2=$(awk -f scripts/get_aligned_reads.awk {input.calStats});"
		"n3=$(awk -f scripts/get_aligned_reads.awk {input.inputStats});"
		"n4=$(awk -f scripts/get_aligned_reads.awk {input.inputCalStats});"
		'n=$(python -c "print(($n4 * $n1) / ($n2 * $n3))");'
		"echo $n > {output}"		

rule sort_sam:
	input:
		sam=DATADIR + "/align/{sample}.sam"
	output:
		sorted_bam=temp(DATADIR + "/align/{sample}_sorted.bam")
		#sorted_bam=DATADIR + "/align/{sample}_sorted.bam"
	log:
		LOGDIR + "/picard/sortSam_{sample}.log"
	threads: 
		1
	resources:
		mem_mb=get_resource("gatk","mem_mb"),
		walltime=get_resource("gatk","walltime")
	shell:
		'''
		gatk SortSam --java-options "-Xmx{resources.mem_mb}M" \
		-I {input.sam} \
		-O {output.sorted_bam} \
		--SORT_ORDER coordinate |& tee {log}
		'''


rule remove_duplicates:
	input:
		sorted_bam=DATADIR + "/align/{sample}_sorted.bam"
	output:
		nodup_bam=DATADIR + "/align/{sample}_final.bam",
		metrics=DATADIR + "/align/stats/picardMarkDup_{sample}.txt"
	threads: 
		1
	resources:
		mem_mb=get_resource("gatk", "mem_mb"),
		walltime=get_resource("gatk","walltime")
	log:
		LOGDIR + "/gatk/markDup_{sample}.log"
	shell:
		'''
		gatk MarkDuplicates --java-options "-Xmx{resources.mem_mb}M" \
		-I {input.sorted_bam} \
		-O {output.nodup_bam} \
		-M {output.metrics} |& tee {log}
		'''

rule index_bam:
	input:
		DATADIR + "/align/{sample}_final.bam"
	output:
		DATADIR + "/align/{sample}_final.bai"
	threads: 1

	resources:
		mem_mb=get_resource("gatk", "mem_mb"),
		walltime=get_resource("gatk","walltime")
	log:
		LOGDIR + "/gatk/index_{sample}.log"
	shell:
		'''
		gatk BuildBamIndex --java-options "-Xmx{resources.mem_mb}M" \
		-I {input} |& tee {log}
		'''


rule create_bigWig:
	input:
		nodup_bam=DATADIR + "/align/{sample}_final.bam",
		bam_index=DATADIR + "/align/{sample}_final.bai"
	output:
		bw=RESDIR + "/bw/{sample}_RPKM.bw"
	params:
		genomeSize= lambda wildcards: expand("{genome_size}", 
			genome_size=data.Genome_size[data.Samples==wildcards.sample].values[0])
	threads:
		get_resource("create_bigWig", "threads")
	conda:
		"envs/deeptools.yaml"
	resources:
		mem_mb=get_resource("create_bigWig","mem_mb"),
		walltime=get_resource("create_bigwig","walltime")
	log:
		LOGDIR + "/deeptols/bamCoverage_{sample}.log"
	shell:
		'''
		bamCoverage --effectiveGenomeSize {params.genomeSize} \
		--normalizeUsing RPKM -p {threads} \
		-b {input.nodup_bam} -o {output.bw} |& tee -a {log} 
		'''

rule create_bigWig_scaled:
	input:
		nodup_bam=DATADIR + "/align/{sample}_final.bam",
		bam_index=DATADIR + "/align/{sample}_final.bai",
		scaleFactor=DATADIR + "/align/{sample}_ORI_factor.txt"
	output:
		bw=RESDIR + "/bw/{sample}_RPKM_scaled.bw"
	params:
		genomeSize= lambda wildcards: expand("{genome_size}", 
			genome_size=data.Genome_size[data.Samples==wildcards.sample].values[0])
	threads:
		get_resource("create_bigWig", "threads")
	conda:
		"envs/deeptools.yaml"
	resources:
		mem_mb=get_resource("create_bigWig","mem_mb"),
		walltime=get_resource("create_bigwig","walltime")
	log:
		LOGDIR + "/deeptols/bamCoverage_{sample}_scaled.log"
	shell:
		'''
		n=$(cat {input.scaleFactor});
		bamCoverage --effectiveGenomeSize {params.genomeSize} \
		--normalizeUsing CPM --scaleFactor $n -p {threads} \
		-b {input.nodup_bam} -o {output.bw} |& tee -a {log} 
		'''			

rule macs2_callpeak:
	input:
		treatBam= DATADIR + '/align/{sample}_final.bam',
		inputBam= lambda wildcards: expand(
					DATADIR+'/align/{inputSample}_final.bam',
			inputSample=data.Input[data.Samples == wildcards.sample].values[0])
	output:
		pred=RESDIR+'/macs/{sample}_predictd.txt',
		peaks=RESDIR + '/macs/{sample}_peaks.xls'
	threads:
		get_resource("macs2", "threads")
	resources:
		mem_mb=get_resource("macs2", "mem_mb"),
		walltime=get_resource("macs2", "walltime")
	log:
		LOGDIR + "/macs/{sample}.log"
	run:
		OUTDIR=RESDIR+'/macs'
		command='scripts/macs2_callPeaks.sh {{input.treatBam}} {{input.inputBam}} hs \
		{{wildcards.sample}} {outDir} &> {{log}}'.format(outDir=OUTDIR)
		print("=========\n"+ command + "\n=========" )
		shell(command)

## For rule compute matrix we need to generate lists with filenames and labels
matrixFiles = data.Samples[data.Protein != 'input'].unique()
matrixLabels = [re.sub("_S.+$","",f) for f in matrixFiles]

rule compute_matrix:
	input:
		bw = expand(RESDIR + "/bw/{MatFile}_RPKM_scaled.bw",
				  MatFile=matrixFiles),
		regions = refSeq_genes_path["hg19"]
	output:
		matrix = RESDIR + "/deeptools/matrix_all_samples.gz",
		sortRegions = RESDIR + "/deeptools/regions_all_samples_TSS_sorted.bed"
	threads:
		get_resource("compute_matrix", "threads")
	params:
		command='computeMatrix reference-point --referencePoint TSS \
			--scoreFileName {{input.bw}} \
			----regionsFileName {{input.regions}} \
			--upstream 5000 --downstream 5000 \
			--binSize 20 \
			--sortRegions descend \
			--sortUsing mean \
			--missingDataAsZero \
			--samplesLabel {Labels} \
			--p {{threads}} \
			--outFileName {{output.matrix}} \
			--outFileSortedRegions {{output.sortRegions}}'.format(
				Labels=matrixLabels)
	conda:
		"envs/deeptools.yaml"
	resources:
		mem_mb=get_resource("compute_matrix", "mem_mb"),
		walltime=get_resource("compute_matrix", "walltime")
	shell:
		"{params.command}"




